## [П]|[РС]|(РП) Вычитание фона

Из-за своей простоты и т.к. камера, как правило, зафиксирована, *вычитание фона*, вероятно, наиболее фундаментальная операция обработки изображения из видеопотока. Для выполнения операции вычитания фона для начала необходимо "изучить" модель фона. Однажды изученная *модель фона* сравнивается с текущим изображением с последующим исключением известных частей фона. Объекты, оставшиеся после исключения, предположительно будут являться новыми объектами переднего плана.

"Фон" является плохо определенным понятием, которое изменяется в зависимости от применения. Например, в случае рассмотрения шоссе, возможно, обычный транспортный поток следует считать фоном. Как правило, за фон принимаются любые статические или периодически движущие части сцены, которые остаются неизменными или периодическими в течение интересующего периода времени. Группа может иметь изменяющиеся во времени компоненты, такие, например, как деревья, которые развиваются от ветра утром и вечером, но неподвижны в полдень. Две распространённые, но существенно различающиеся категории окружения, которые могут встретиться, это сцены внутри и снаружи помещений. Возникает интерес в инструментах, которые помогли бы разобраться с обеими категориями окружения. В данной главе вначале будут рассмотрены недостатки типичных моделей фона, а затем будут рассмотрены модели сцен высших порядков. Далее будет представлен быстрый метод, который в основном хорош для статичных фоновых сцен внутри помещений, освещение которых практически не меняется. Затем будет рассмотрен метод "кодовых книг", который немного медленнее, однако может работать в сценах и внутри и снаружи помещений; этот метод подходит для периодических движений (таких, как раскачивающихся на ветру деревьев) и для медленного или периодически изменяемого освещения. Этот метод так же устойчив при изучении фона, даже когда есть случайные движущиеся объекты переднего плана. Ранее эта тема уже была затронута во время обсуждения связанных компонентов (впервые в главе 5) в контексте обнаружения объекта переднего плана. И в конце главы, будет представлен сравнительный анализ быстрого метода исключения фона с методом "кодовой книги".


### Слабые стороны исключения фона

Хотя методы моделирования фона, упомянутые в данной главе, работают достаточно хорошо для простых сцен, однако, они страдают от предположения, которое часто нарушается: что все пиксели независимы. Рассматриваемые методы обучают модель изменения пикселей без учета соседних пикселей. Для принятия окружающих пикселей во внимание, необходимо изучить модель, состоящую из нескольких частей; простым примером такой модели является расширенная основная модель независимых пикселей путем элементарного включения чувствительных к яркости соседних пикселей. В этом случае используется яркость соседних пикселей, чтобы различать случаи, когда значение соседнего пикселя будет относительно ярким или тусклым. В связи с этим существует две модели для конкретного пикселя: одна для случая, когда соседние пиксели более яркие, а другая для случая, когда соседние пиксели более тусклые. В общем, имеется модель, которая принимает во внимание окружающий *контекст*. Однако это приводит к возрастанию используемой памяти в 2 раза и количеству операций вычислений, так как возникает потребность в значениях для случаев, когда окружающие пиксели либо ярче, либо более тусклые. Помимо этого, необходимо в два раза больше данных, чтобы заполнить эти две модели состояний. Можно обобщить идею "высокого" и "низкого" контекстов в многомерную гистограмму интенсивности конкретного и соседнего пикселей, а также сделать её ещё более сложной, выполнив это за несколько временных шагов. При этом стоит принимать во внимание, что более сложная в пространстве и времени модель требует ещё больше памяти, собираемых данных и вычислительных ресурсов.

Из-за дополнительных расходов использование более сложных моделей, как правило, стараются избегать. Для более эффективного распоряжения ресурсами, можно избавляться от *ложноположительных* пикселей, которые оказывают влияние в тех случаях, когда нарушается предположение о независимости пикселей. Избавление принимает форму операций обработки изображений (как правило, *cvErode()*, *cvDilate()* и *cvFloodFill()*), которые исключают ложноположительные пиксели. Ранее эти операции уже были рассмотрены (глава 5) в контексте поиска больших и компактных (компактные - это математический термин, который не имеет ничего общего с размером) *связанных компонентов* в данных с наличием шума. В данной главе связанные компоненты так же будут использованы, а на данный момент ограничимся подходом, который предполагает независимое изменение пикселей.


### Моделирование сцены

Итак, каким же образом отделить фон от переднего плана? Например, если ведется наблюдение за стоянкой и на парковку въезжает автомобиль, то он будет являться новым объектом переднего плана. Но должен ли он оставаться объектом переднего плана навсегда? А как насчет перемещенного мусорного бака? Он будет отображаться на переднем плане в двух местах: туда, куда переместили и "дырой" в месте, откуда он был перемещен. Как объяснить эту разницу? И ещё, как долго мусорный бак (или "дыра") остается объектом переднего плана? Если смоделировать темную комнату, и кто-то вдруг включит свет, должна ли вся комната стать объектом переднего плана? Чтобы ответить на все эти вопросы, необходима высокоуровневая модель "сцены", которая определяет несколько уровней между состояниями переднего плана и фона, а также временной метод медленной передачи неподвижных объектов переднего плана фону. К тому же требуется определять и создавать новую модель при глобальных изменениях в сцене.

В общем, модель сцены может содержать несколько слоев, от "нового переднего плана" до старого переднего плана и так вплоть до фона. Также должно быть реализовано детектирование движения таким образом, чтобы при перемещении объекта можно было идентифицировать "позитивную" часть (новое местоположение) и "негативную" часть (старое местоположение, "дыра").

Таким образом, новый объект переднего плана должен быть перемещен на уровень "новые объекты переднего плана" и отмечен как позитивный объект или как дыра. В районах, где нет объектов переднего плана, можно продолжать обновление модели фона. Если объект переднего плана не перемещался в течение заданного участка времени, то он будет перемещен на уровень "старые объекты переднего плана", где пиксельная статистика предварительно изучается до тех пор, пока изучаемая модель не присоединиться к модели фона.

Для отслеживания глобальных изменений, таких, как включение освещения в помещении, необходимо использовать глобальную разность кадров. Например, если одновременно изменениям подверглось большое количество пикселей, тогда можно классифицировать это скорее, как глобальное изменение, а не локальное, а затем переключиться на использование модели для новой ситуации.


### Срез пикселей

Прежде, чем перейти к моделированию пиксельных изменений, необходимо получить представление о том, как изменяются пиксели изображения во времени. Рассмотрим случай, когда камера следит за деревом на улице, которое раскачивается на ветру. На рисунке 9-1 показано, как выглядят пиксели выбранного линейного сегмента на протяжении 60 кадров. Зададимся целью получения модели этих колебаний. Однако, прежде, сделаем небольшое отступление, чтобы обсудить способ получения этой линии, потому что в целом это полезный прием для генерации признаков и для отладки. 

![Рисунок 9-1 не найден](Images/Pic_9_1.jpg)

Рисунок 9-1. Колебание пикселей линии на протяжении 60 кадров из сцены раскачивающегося дерева: некоторые темные области (вверху слева) довольно таки стабильны, тогда как в области движущихся ветвей (вверху в центре) могут изменяться в широких приделах

В OpenCV имеются функции, которые позволяют с легкостью получить произвольную линию пикселей. Это функции *cvInitLineIterator()* и *CV_NEXT_LINE_POINT()*. Прототип функции *cvInitLineIterator()*:

```cpp
int cvInitLineIterator(
     const CvArr*       image
    ,CvPoint            pt1
    ,CvPoint            pt2
    ,CvLineIterator*    line_iterator
    ,int                connectivity    = 8
    ,int                left_to_right   = 0
);
```

Исходное изображение *images* может иметь любой тип или количество каналов. Точки *pt1* и *pt2* являются концами линейного сегмента. Итератор *line_iterator* отвечает за перемещение между точками вдоль линии. В случае использования многоканальных изображений, каждый вызов *CV_NEXT_LINE_POINT()* перемещает *line_iterator* к следующему пикселю. Для получения доступа ко всем каналам необходимо использовать *line_iterator.ptr[0]*, *line_iterator.ptr[1]* и так далее. Связность *connectivity* может быть равна 4 (линия может совершать шаги вверх, вниз, влево, вправо) или 8 (линия может дополнительно делать шаги по диагоналям). Если *left_to_right* равен 0 (false), тогда *line_iterator* совершает шаги от *pt1* до *pt2*; иначе, шаги будут осуществляться от крайней левой точки к крайней правой. (Флаг *left_to_right* был введен из-за того, что дискретная линия, проведенная от *pt1* к *pt2*, не всегда соответствует линии, проведенной от *pt2* к *pt1*. Таким образом, установка этого флага дает пользователю получить точную растеризацию в независимости от последовательности *pt1*, *pt2*). Функция *cvInitLineIterator()* возвращает число точек, которые были пройдены для этой линии. Сопутствующий макрос *CV_NEXT_LINE_POINT(line_iterator)* перемещает итератор от одного пикселя к другому.

Прервемся от обсуждения и посмотрим на то, как этот метод может быть использован для извлечения некоторых данных из файла (пример 9-1). При этом переосмыслим рисунок 9-1 с точки зрения полученных данных из файла.

Пример 9-1. Чтение RGB значений всех пикселей одной строки файла и сохранение этих значений в трех отдельных файлах

```cpp
// Сохранение на диск линейного сегмента из BGR пикселей от p1 до p2
//
CvCapture*      capture = cvCreateFileCapture( argv[1] );
int             max_buffer;
IplImage*       rawImage;
int             r[10000], g[10000], b[10000];
CvLineIterator  iterator;

FILE *fptrb = fopen("blines.csv", "w");  // Создание файлов для сохранения 
FILE *fptrg = fopen("glines.csv", "w");  // каждого из каналов в отдельности
FILE *fptrr = fopen("rlines.csv", "w");

// Главный цикл обработки
//
for(;;) {

    if( !cvGrabFrame( capture ))
        break;

    rawImage = cvRetrieveFrame( capture );
    max_buffer = cvInitLineIterator(rawImage,pt1,pt2,&iterator,8,0);
    for(int j=0; j < max_buffer; j++){

    // Запись значений
    // 
    fprintf(fptrb, "%d,", iterator.ptr[0]); // синий
    fprintf(fptrg, "%d,", iterator.ptr[1]); // зеленый
    fprintf(fptrr, "%d,", iterator.ptr[2]); // красный

    iterator.ptr[2] = 255; // Маркировка этого образца красным

    CV_NEXT_LINE_POINT(iterator); // Переход к следующему пикселю
}

// Вывод данных по строкам
//
fprintf(fptrb, "/n"); fprintf(fptrg,"/n"); fprintf(fptrr, "/n");
}

// Очистка
//
fclose(fptrb); fclose(fptrg); fclose(fptrr);
cvReleaseCapture( &capture );
```

Получить линию выборки можно ещё проще, а именно:

```cpp
int cvSampleLine(
     const CvArr*   image
    ,CvPoint        pt1
    ,CvPoint        pt2
    ,void*          buffer
    ,int            connectivity = 8
);
```

Эта функция просто обертка функции *cvInitLineIterator()* вместе с макросом *CV_NEXT_LINE_POINT(line_iterator)*. Она производит выборку от *pt1* до *pt2*; затем передает указатель *buffer* нужного типа и длиной ![Формула 9-1 не найдена](Images/Frml_9_1.jpg). Так же как и линейный итератор, *cvSampleLine()* пошагово проходит по каждому каналу каждого пикселя многоканального изображения, прежде, чем переместиться к следующему пикселю. Функция возвращает число элементов *buffer*.

Теперь можно переходить к рассмотрению методов моделирования колебаний пикселей, рассмотренных на рисунке 9-1. По мере продвижения от простой к более сложной модели, будут представлены лишь те, которые работают в режиме реального времени и в рамках разумных ограничений по памяти.


### Выявление отличительных признаков между кадрами

Самым простым способом вычитания фона является вычитание одного кадра из другого (возможно расположенного несколько позже) с последующим выделением любой разницы, которая "достаточно большая" на переднем плане. Этот процесс стремиться поймать границы движущихся объектов. Для простоты рассмотрим три одноканальных изображения: *frameTime1*, *frameTime2* и *frameForeground*. Во *frameTime1* поместим предыдущий кадр в оттенках серого, а в *frameTime2* текущий кадр в оттенках серого. Затем, воспользовавшись *cvAdsDiff()*, вычислим (в абсолютных значениях) разницу между передними планами и поместим результат в *frameForeground*.

```cpp
cvAbsDiff(
     frameTime1
    ,frameTime2
    ,frameForeground
);
```

Так как в значениях пикселя всегда присутствует шум и колебания, необходимо игнорировать (устанавливать в 0) малые различия (например, меньше 15) и помечать остальные как большие различия (устанавливать в 255).

```cpp
cvThreshold(
     frameForeground
    ,frameForeground
    ,15
    ,255
    ,CV_THRESH_BINARY
);
```

Таким образом изображение *frameForeground* будет содержать кандидатов на объекты переднего плана, отмеченные пикселями со значениями равными 255 и фон, отмеченный пикселями со значениями 0. Теперь, как уже было сказано ранее, необходимо избавиться от мелких шумовых областей; для этого можно использовать функцию *cvErode()* или связанную компоненту. Для цветных изображений данный подход также может быть использован, но необходимо применять его к каждому каналу в отдельности с последующим соединением их обратно с помощью *cvOr()*. Этот метод является слишком простым для большинства приложений и позволяет отмечать лишь области движения. Для получения более эффективной модели фона, необходимо накапливать некоторую статистику о средних значениях и средних различиях между пикселями сцены. Забегая немного вперед, можно посмотреть примеры различий между кадрами на рисунке 9-5 и рисунке 9-6 в разделе "Быстрый тест".


### Метод усреднения фона

Основу метода усреднения для создания модели фона составляют среднее значение и стандартное отклонение (или аналогичная, но более быстро вычисляемая, средняя разница) каждого пикселя.

Рассмотрим линию пикселей с рисунка 9-1. Вместо построения одной последовательности значений для каждого пикселя, можно представить изменения каждого пикселя на протяжении всего видеофайла с точки зрения среднего значения и средней разности (рисунок 9-2). В этом же видео объект переднего плана (который, по факту, является рукой) перемещается перед камерой. Этот объект на переднем плане не столь яркий, как небо или дерево на фоне. Яркость руки так же показана на рисунке.

![Рисунок 9-2 не найден](Images/Pic_9_2.jpg)

Рисунок 9-2. Данные рисунка 9-1 представлены в виде средних различий: объект (рука), который перемещается перед камерой несколько темнее, что, собственно, и отображает график

Метод усреднения использует четыре функции OpenCV: *cvAcc()* для накопления кадров в течении заданного времени; *cvAbsDiff()* для накопления изменений от кадра к кадру в течении заданного времени; *cvInRange()* для выделения сегментов переднего и заднего планов (после получения модели фона); *cvOr()* для объединения сегментов, полученных от разных цветовых каналов, в единую маску изображения. Так как этот пример содержит значительное количество кода, он будет разбит на смысловые части и каждая часть будет рассмотрена в отдельности.

Вначале создаются указатели на несколько черновых и хранящих статистику изображений, потребность в которых будет на протяжении всего жизненного цикла программы. Возможно, будет полезно отсортировать эти указатели, исходя из типа изображений, на которые они ссылаются.

```cpp
// Глобальное хранилище
// Float, 3-channel images
//
IplImage *IavgF,*IdiffF, *IprevF, *IhiF, *IlowF;
IplImage *Iscratch, *Iscratch2;

// Float, 1-channel images
//
IplImage *Igray1,*Igray2, *Igray3;
IplImage *Ilow1, *Ilow2, *Ilow3;
IplImage *Ihi1, *Ihi2, *Ihi3;

// Byte, 1-channel image
//
IplImage *Imaskt;

// Подсчет количества изображений, которые были изучены 
// для последующего усреднения
// 
float Icount;
```

Затем создается единый вызов для выделения памяти под все необходимые промежуточные изображения. Для удобства будет передано одно изображение (из видео), которое может быть использовано в качестве эталона для калибровки промежуточных изображений.

```cpp
// Это просто эталонное изображения для распределительных 
// целей (используется для калибровки)
//
void AllocateImages( IplImage* I ) {
    CvSize sz = cvGetSize( I );

    IavgF   = cvCreateImage( sz, IPL_DEPTH_32F, 3 );
    IdiffF  = cvCreateImage( sz, IPL_DEPTH_32F, 3 );
    IprevF  = cvCreateImage( sz, IPL_DEPTH_32F, 3 );
    IhiF    = cvCreateImage( sz, IPL_DEPTH_32F, 3 );
    IlowF   = cvCreateImage( sz, IPL_DEPTH_32F, 3 );
    Ilow1   = cvCreateImage( sz, IPL_DEPTH_32F, 1 );
    Ilow2   = cvCreateImage( sz, IPL_DEPTH_32F, 1 );
    Ilow3   = cvCreateImage( sz, IPL_DEPTH_32F, 1 );
    Ihi1    = cvCreateImage( sz, IPL_DEPTH_32F, 1 );
    Ihi2    = cvCreateImage( sz, IPL_DEPTH_32F, 1 );
    Ihi3    = cvCreateImage( sz, IPL_DEPTH_32F, 1 );
    
    cvZero( IavgF );
    cvZero( IdiffF );
    cvZero( IprevF );
    cvZero( IhiF );
    cvZero( IlowF );
    
    Icount = 0.00001; // защита от деления на 0

    Iscratch    = cvCreateImage( sz, IPL_DEPTH_32F, 3 );
    Iscratch2   = cvCreateImage( sz, IPL_DEPTH_32F, 3 );
    Igray1      = cvCreateImage( sz, IPL_DEPTH_32F, 1 );
    Igray2      = cvCreateImage( sz, IPL_DEPTH_32F, 1 );
    Igray3      = cvCreateImage( sz, IPL_DEPTH_32F, 1 );
    Imaskt      = cvCreateImage( sz, IPL_DEPTH_8U,  1 );
    
    cvZero( Iscratch );
    cvZero( Iscratch2 );
}
```

В следующем куске кода представлены процессы накопления фона изображения и накопления абсолютных значений межкадровых разностей изображения (процесс вычисления длится быстрее "прокси" (средняя разница не является математическим эквивалентом стандартному отклонению, но в данном контексте достаточно близко, чтобы давать результаты аналогичного качества) для изучения стандартного отклонения пикселей изображения). Эта операция выполняется, как правило, для 30-1000 кадров, иногда только один кадр от каждой секунды, а иногда и для всех кадров. Процедура вызывается для трехканального изображения глубиной 8 бит.

```cpp
// Изучение статистики фона кадра 
// I - это цветной образец фона, 3-канальный, 8u
//
void accumulateBackground( IplImage *I ) {
    static int first = 1;               // не потокобезопасно
    cvCvtScale( I, Iscratch, 1, 0 );    // конвертация во float

    if( !first ) {
        cvAcc( Iscratch, IavgF );
        cvAbsDiff( Iscratch, IprevF, Iscratch2 );
        cvAcc( Iscratch2, IdiffF );
        Icount += 1.0;
    }

    first = 0;
    cvCopy( Iscratch, IprevF );
}
```

Сначала используется функция *cvCvtScale()* для преобразования необработанного фонового 8-битного трехканального изображения в трехканальное изображение типа float. Затем происходит накопление необработанного float-изображения в *IavgF*. Потом происходит вычисление межкадровой абсолютной разницы при помощи *cvAbsDiff()* и накопление её в изображении *IdiffF*. Каждый раз при накоплении этих изображений увеличивается глобальный счетчик изображений *Icount*, который будет использован позже для усреднения.

Единожды накопив достаточное количество кадров, можно конвертировать их в статистическую модель фона. В результате это позволит вычислить среднее значение и отклонение (абсолютное значение средней разницы) каждого пикселя.

```cpp
void createModelsfromStats() {
    cvConvertScale( IavgF,  IavgF,  (double)(1.0/Icount) );
    cvConvertScale( IdiffF, IdiffF, (double)(1.0/Icount) );

    // Модификация разности, чтобы она всегда была не равна 0
    //
    cvAddS( IdiffF, cvScalar( 1.0, 1.0, 1.0), IdiffF );
    setHighThreshold( 7.0 );
    setLowThreshold( 6.0 );
}
```

В представленном куске кода функция *cvConvertScale()* вычисляет значения среднего и абсолютной разницы за счет деления на число накопленных изображений. В качестве меры предосторожности, предполагается, что средняя разница изображений, по крайней мере, меньше 1; данный показатель необходимо будет изменять во время пороговых преобразований и избегать ситуаций, когда эти два порога могут стать равными.

Обе вспомогательные функции *setHighThreshold()* и *setLowThreshold()* задают порог, основываясь на абсолютном значении межкадровой средней разности. Вызов *setHighThreshold(7.0)* устанавливает порог таким, что при любом значении, которое превысит в 7 раз среднюю абсолютную разность для этого пикселя, указывает на то, что этот пиксель принадлежит переднему плану; аналогичным образом вызов *setLowThreshold(6.0)* устанавливает нижний порог. Значение, лежащее между этими порогами, указывает на принадлежность пикселя к фону. 

```cpp
void setHighThreshold( float scale ) {
    cvConvertScale( IdiffF, Iscratch, scale );
    cvAdd( Iscratch, IavgF, IhiF );
    cvSplit( IhiF, Ihi1, Ihi2, Ihi3, 0 );
}

void setLowThreshold( float scale ) {
    cvConvertScale( IdiffF, Iscratch, scale );
    cvSub( IavgF, Iscratch, IlowF );
    cvSplit( IlowF, Ilow1, Ilow2, Ilow3, 0 );
}
```

И вновь в *setLowThreshold()* и *setHighThreshold()* используется *cvConvertScale()* для перемножения значений перед сложением или вычитанием этих диапазонов по отношению к *IavgF*. Это действие устанавливает *IhiF* и *IlowF* диапазоны для каждого канала изображения при помощи *cvSplit()*.

Как только появляется модель фона в комплекте с верхним и нижним порогами можно использовать это для сегментации изображения на передний план (то, что не "указано" на фоновом изображении) и фон (все, что находится между верхним и нижним порогами фоновой модели). Сегментация выполняется при помощи следующего куска кода:

```cpp
// Создание бинарного изображения: маска 0,255, где 255 указывает на передний план
// I - входное трехканальное 8-битное изображение
// Imask - одноканальное 8-битное изображение маски, которое должно быть создано
// 
void backgroundDiff( IplImage *I, IplImage *Imask ) {
    cvCvtScale(I,Iscratch,1,0); // Конвертирование во float
    cvSplit( Iscratch, Igray1,Igray2,Igray3, 0 );

    // Channel 1
    //
    cvInRange(Igray1,Ilow1,Ihi1,Imask);

    // Channel 2
    //
    cvInRange(Igray2,Ilow2,Ihi2,Imaskt);
    cvOr(Imask,Imaskt,Imask);

    // Channel 3
    //
    cvInRange(Igray3,Ilow3,Ihi3,Imaskt);
    cvOr(Imask,Imaskt,Imask);

    // Инвертирование результата
    //
    cvSubRS(Imask, cvScalar(255), Imask);
}
```

Вначале эта функция преобразует исходное изображение *I* (изображение, которое необходимо сегментировать) в вещественное изображение с помощью функции *cvCvtScale()*. Затем выполняется конвертирование трехканального изображения в одноканальные при помощи *cvSplit()*. Потом эти одноканальные изображения проходят проверку на соответствие верхней и нижней амплитуде среднего фонового пикселя при помощи функции *cvInRange()*, которая устанавливает значения пикселей 8-битного изображения *Imask* в *max* (255), когда данное значение лежит в указанном диапазоне, иначе в 0. Используя логическую функцию OR для каждого канала, осуществляется перенос результатов сегментации на изображение *Imask*; в результате любые сильные различия в любом канале можно рассматривать как свидетельство принадлежности пикселя к объекту переднего плана. В заключении, происходит инвертирование *Imask* при помощи *cvSubRS()*, потому что передний фон должен содержать значения вне диапазона, а не в диапазоне. Изображение маски является результатом.

Так же необходимо освободить память, занимаемую изображениями по завершении использования фоновой модели:

```cpp
void DeallocateImages() {
    cvReleaseImage( &IavgF );
    cvReleaseImage( &IdiffF );
    cvReleaseImage( &IprevF );
    cvReleaseImage( &IhiF );
    cvReleaseImage( &IlowF );
    cvReleaseImage( &Ilow1 );
    cvReleaseImage( &Ilow2 );
    cvReleaseImage( &Ilow3 );
    cvReleaseImage( &Ihi1 );
    cvReleaseImage( &Ihi2 );
    cvReleaseImage( &Ihi3 );
    cvReleaseImage( &Iscratch );
    cvReleaseImage( &Iscratch2 );
    cvReleaseImage( &Igray1 );
    cvReleaseImage( &Igray2 );
    cvReleaseImage( &Igray3 );
    cvReleaseImage( &Imaskt );
}
```

В результате был рассмотрен простой метод изучения фона сцены, и сегментация объектов переднего плана. Этот метод показывает хорошие результаты, когда сцена не содержит движущихся объектов фона (например, развивающихся занавесок или деревьев). Так же предполагается, что освещение остается постоянным (например, в помещении неподвижных сцен). Результаты работы метода представлены на рисунке 9-5.

**Накопление среднего значения, дисперсии и ковариации**

Только что рассмотренный метод усреднения при вычитании фона использовал одну аккумулирующую функцию *cvAcc()*. Она принадлежит к группе вспомогательных функций и служит для накопления (аккумулирования) сумм изображений, квадратов изображений, перемноженных изображений и усредненных изображений, из которых можно вычислить базовую статистику (среднее значение, дисперсию, ковариацию) сцены в целом или в частности. В этом разделе также будут рассмотрены и другие функции из этой группы.

Изображение во всех указанных функциях должно иметь одинаковые размеры по высоте и ширине. В каждой из функций, исходные изображения, именуемые *image*, *image1* и *image2*, могут быть одно- или трехканальными (8-бит на канал) или вещественным (32F) массивом изображений. Конечные изображения, именуемые *sum*, *sqsum* и *acc*, могут быть массивами одинарной точности (32F) или двойной (64F). На аккумулирующую функцию изображение *mask* (если имеется) накладывает ограничения на обработку только тех мест, где элементы маски отличны от нуля.

**Определение среднего значения**. Наиболее простой метод нахождения среднего заключается в сложении всего набора изображений с использованием *cvAcc()* и последующим делением на общее количество изображений.

```cpp
void cvAcc(
     const Cvrr*    image
    ,CvArr*         sum
    ,const CvArr*   mask = NULL
);
```

Альтернативный метод заключается в использование скользящего среднего:

```cpp
void cvRunningAvg(
     const CvArr*   image
    ,CvArr*         acc
    ,double         alpha
    ,const CvArr*   mask = NULL
);
```

Скользящее среднее может быть найдено по следующей формуле:

![Формула 9-2 не найдена](Images/Frml_9_2.jpg)

Для константного значения α, скользящее среднее не будет эквивалентно результатам суммирования с помощью *cvAcc()*. Чтобы в этом убедиться, просто просуммируем числа (2, 3 и 4) и установим α = 0.5. Если просуммировать их при помощи *cvAcc()*, то сумма была бы равна 9, а среднее значение 3. Если же просуммировать их при помощи *cvRunningAverage()*, тогда первая сумма даст 0,5×2+0,5×3=2,5, а последующее добавление третьей составляющей даст результат 0,5×2,5+0,5×4=3,25. Причина, по которой второе число больше заключается в том, что последние вклады дают большие веса, чем более отдаленные во времени. Скользящее среднее зачастую называют *трекером*. Параметр α устанавливает время, на протяжении которого предыдущий кадр будет оказывать влияние.

**Определение дисперсии**. Возможность аккумулировать квадраты изображений позволяет быстро вычислять дисперсии отдельных пикселей.

```cpp
void cvSquareAcc(
     const CvArr*   image
    ,CvArr*         sqsum
    ,const CvArr*   mask = NULL
);
```

Дисперсия конечной последовательности определяется по формуле:

![Формула 9-3 не найдена](Images/Frml_9_3.jpg)

где ![Формула 9-4 не найдена](Images/Frml_9_4.jpg) – среднее значение x для всех образцов N. Слабой стороной данной формулы является то, что для её вычисления требуется выполнять два прохода по изображению: один проход для вычисления ![Формула 9-4 не найдена](Images/Frml_9_4.jpg), второй проход для вычисления ![Формула 9-5 не найдена](Images/Frml_9_5.jpg). Ниже представленная формула работает столь же хорошо, как и ранее представленная:

![Формула 9-6 не найдена](Images/Frml_9_6.jpg)

Используя данную формулу, можно накопить за один проход и значения пикселей и их квадраты. Таким образом, дисперсия пикселя равна разности между средним арифметическим квадратов и квадратом среднего арифметического.

**Определение ковариации**. Увидеть, как меняется изображение с течением времени, можно путем выбора определенного *лага* (отставания), а затем умножить текущее изображение на изображение из прошлого, которое соответствует данному лагу (отставанию). Функция *cvMultiplyAcc()* будет выполнять попиксельное перемножение двух изображений, а затем добавлять результат "нарастающим итогом" в *acc*:

```cpp
void cvMultiplyAcc(
     const CvArr*   image1
    ,const CvArr*   image2
    ,CvArr*         acc
    ,const CvArr*   mask = NULL
);
```

Для ковариации есть формула, аналогичная формуле для дисперсии. Эта формула выполняет вычисления за один проход за счет алгебраических преобразований стандартной формулы:

![Формула 9-7 не найдена](Images/Frml_9_7.jpg)

где *x* - изображение в момент времени *t*, *y* - в момент времени *t - d* (*d* - отставание).

Можно использовать аккумулирующие функции, описанные здесь, чтобы создать несколько моделей фона, основанных на статистике. В иной литературе можно найти различные вариации базовой модели, используемой в приведенном примере. Как правило, при применении данной модели возникает необходимость в расширении данной модели до более специализированной версии. К примеру, простым улучшением будет являться адаптивная бинаризация, способная приспосабливаться к изменению глобальных переменных.


### Усовершенствованная модель фона

Многие фоновые сцены содержат движущиеся объекты, такие как раскачивающиеся деревья на ветру, включенные вентиляторы, развевающиеся занавески и т.д. Зачастую такие сцены также содержат переменное освещение из-за движущихся облаков или дверей и окон по-разному пропускающие свет.

Хороший способ справиться с этим заключается в применении временной модели для каждого пикселя или группы пикселей. Такой вид модели хорошо взаимодействует с временными колебаниями, однако, большим минусом является то, что данная модель потребляет большое количество памяти. Если использовать 2 секунды предыдущей информации с частотой 30 Hz, то потребуется 60 образцов для каждого пикселя. Затем результирующая модель каждого пикселя должна закодировать то, что она изучила 60 различных адаптивных *весов*. Зачастую возникает потребность в накоплении фоновой статистики значительно дольше, чем 2 секунды; это означает, что такие методы нецелесообразно использовать на современном оборудовании.

Для максимально возможного увеличения производительности адаптивной фильтрации, необходимо позаимствовать метод сжатия видео и попытаться сформировать *кодовую книгу* для воспроизведения наиболее важных положений на фоне. Простой путь сделать это заключается в сравнении нового значения определенного пикселя с его предыдущим значением. Если значение довольно таки близко к предыдущему, то моделируется искажение цвета. Иначе должна создаваться новая группа из цветов, ассоциированная с этим пикселем. Результат может быть представлен как набор *blobs* (капель), перемещающихся в пространстве RGB, где каждый *blob*, представляющий отдельную часть, рассматривается как возможная составляющая фона. 

На практике выбор пространства RGB не является оптимальным. Практически всегда лучше использовать цветовое пространство, у которого ось совпадают с яркостью, такое как YUV. (Выбор YUV является наиболее распространенным, хотя пространство HSV, где V – яркость, так же является неплохим выбором.) Причина использования таких цветовых пространств заключается в том, что эмпирически большинство вариаций фона принадлежат оси яркости, а не цветовой оси.

Теперь рассмотрим, как моделировать *blobs*. Собственно, это можно сделать так же, как и раньше в простой модели. Например, можно выбрать модель *blob* как Гауссовы кластеры со средним или ковариацией. Оказывается, в простейшем случае, *blobs* являются просто *boxes* с изученной степенью каждой из трех осей цветового пространства. Этот простейший метод с точки зрения требуемой памяти и вычислительных ресурсов для определения того, будет ли вновь наблюдаемый пиксель внутри любого из изученных *boxes*.

Для понимания того, что такое кодовая книга, воспользуемся простейшим примером (рисунок 9-3). Кодовая книга состоит из *boxes*, которые разрастаются по мере покрытия значений, наблюдаемые во времени. В верхней части рисунка 9-3 отображено колебание сигналов во времени. В нижней части рисунка отображено формирование *box* из новых значений с последующим разрастанием за счет поглощения соседних значений. Если значение слишком далеко, то формируется новый *box* и процесс повторяется.

![Рисунок 9-3 не найден](Images/Pic_9_3.jpg)

Рисунок 9-3. Кодовая книга — это просто *boxes*, разделяющие значения яркости: *box* формируется для поглощения нового значения и медленно разрастается за счет близлежащих значений; если значение лежит далеко, то формируется новый *box*

Для изучаемой модели фона будет рассмотрена кодовая книга, покрывающая три измерения: три канала каждого пикселя изображения. Рисунок 9-4 визуализирует (измерение интенсивности) кодовую книгу для шести различных пикселей, извлеченных из данных рисунка 9-1. Этот метод может взаимодействовать с пикселями, уровни которых резко изменяются (например, пиксели, относящиеся к листьям деревьев обдуваемые ветром или синему небу позади этих деревьев, которые могут быть представлены различными цветами). При помощи данного более точного метода моделирования можно обнаружить объекты переднего плана, которые имеют значения, лежащие между значениями пикселей. Сравнение, показанное на рисунке 9-2, доказывает, что метод усреднения не может выделить значения, соответствующие руке (показано прерывистой линией), из пиксельных колебаний. Забегая немного вперед, стоит отметить тот факт, что производительность кодовой книги в сравнении с методом усреднения (рисунок 9-7) выше.

![Рисунок 9-4 не найден](Images/Pic_9_4.jpg)

Рисунок 9-4. Часть колебаний яркости изученных записей кодовой книги для шести выбранных пикселей (показаны вертикальными *box*): *boxes* кодовой книги собирают пикселы, которые принимают одно из дискретных значений, в результате чего лучше моделируется модель дискретных распределений; это позволяет обнаружить руку, как объект переднего плана (показано на рисунке точечной линией), среднее значение которого лежит между значениями пикселей, принадлежащих фону. В данном случае кодовая книга имеет только одно измерение и может представлять только колебания яркости

В методе кодовой книги для изучения модели фона каждый *box* определяется двумя порогами (*max* и *min*) для каждой цветовой оси. Эти пороги границ *box* расширяются (*max* становится больше, *min* становится меньше), если новые фоновые образцы попадут внутрь изученных границ (*learnHigh* и *learnLow*) выше *max* или ниже *min* соответственно. Если новый фоновый образец попадает вне границ *box* и его изученных порогов, тогда будет создан новый *box*. В режиме *вычитания фона* используются пороги *maxMod* и *minMod*, которые свидетельствуют о том, что если пиксель "достаточно близок" к *min* и *max* границе *box*, тогда можно считать, что он внутри *box*. Второй порог позволяет регулировать модели под конкретные условия.

**Структуры**

Теперь настало время для более детального разбора алгоритма кодовой книги. Для начала необходимо создать структуру кодовой книги, которая будет просто указывать на группу *boxes* пространства YUV:

```cpp
typedef struct code_book {
    code_element    **cb;
    int             numEntries;
    int             t;  // количество обращений
} codeBook;
```

*numEntries* - количество записей в кодовой книге. Переменная *t* подсчитывает число точек, накопленных от начала или с последней операции очищения. Описание структуры элемента представлено ниже:

```cpp
#define CHANNELS 3

typedef struct ce {
    uchar learnHigh[CHANNELS];  // Верхний порог обучения
    uchar learnLow[CHANNELS];   // Нижний порог обучения
    uchar max[CHANNELS];        // Верхняя граница box
    uchar min[CHANNELS];        // Нижняя граница box
    int t_last_update;          // Позволяет убирать устаревшие записи
    int stale;                  // max negative run (продолжительный период неактивности)
} code_element;
```

Каждая запись кодовой книги расходует 4 байта на канал плюс 2 или *CHANNELSx4+4+4* байт (20 байт при использовании трех каналов). Можно установить *CHANNELS* в любое положительное целое значение меньшее или равное числу каналов цветного изображения, но, как правило, используется 1 ("Y" или только яркость) или 3 (YUV, HSV). В этой структуре для каждого канала *max* и *min* - это границы кодовой книги. Параметры *learnHigh* и *learnLow* - это пороговые значения, управляющие созданием новых записей, а именно: новая запись будет создана, если попадется новый пиксель не лежащий в диапазоне *min - learnLow* и *max + learnHigh* в каждом из каналов. Время последнего обновления *t_last_update* и *stale* используются для удаления редко используемых записей кодовой книги во время обучения. Теперь можно переходить к изучению функций, которые используют эту структуру для изучения динамически меняющегося фона.

**Изучение фона**

Предположим, имеется один объект *codeBook* с *code_elements* для каждого пикселя. Тогда для всего изображения, необходим массив *codeBook*, который по длине равен числу пикселей изображения. Для каждого пикселя вызывается функция *update_codebook()* ровным счетом столько раз, чтобы охватить соответствующие изменения в фоне. Процесс обучения может периодически обновляться, а функция *clear_stale_entries()* может быть использована для обучения модели фона в присутствии (небольшого числа) движущихся объектов. Это становиться возможным за счет удаления редко используемых "устаревших" записей, созданных благодаря движущимся объектам. Реализация *update_codebook()* представлена далее:

```cpp
//////////////////////////////////////////////////////////////
// int update_codebook(uchar *p, codeBook &c, unsigned cbBounds)
// Обновление записей codebook за счет указателя на новые данные
//
// p            Указатель на пиксель YUV
// c            Codebook для этого пикселя
// cbBounds     Изучаемые границы codebook (Rule of thumb: 10)
// numChannels  Число каналов
//
// NOTES:
// 	cvBounds должна иметь длину равную numChannels
//
// RETURN
// 	codebook index
//
int update_codebook(
     uchar*      p
    ,codeBook&   c
    ,unsigned*   cbBounds
    ,int         numChannels
) {
    unsigned int high[3], low[3];
    for(n = 0; n < numChannels; n++) {
        high[n] = *(p+n) + *(cbBounds+n);
        
        if(high[n] > 255) {
            high[n] = 255;
        }
        
        low[n] = *(p+n) - *(cbBounds+n);
        
        if(low[n] < 0) {
            low[n] = 0;
        }
    }
    
    int matchChannel;

    // Проверка соответствия существующему кодовому слову
    //
    for(int i = 0; i < c.numEntries; i++) {
        matchChannel = 0;
        for(n = 0; n < numChannels; n++) {
            // Found an entry for this channel
            if( (c.cb[i]->learnLow[n] <= *(p+n)) 
                && (*(p+n) <= c.cb[i]->learnHigh[n])
            ) {
                matchChannel++;
            }
        }

        // Если запись найдена
        // 
        if(matchChannel == numChannels) {
            c.cb[i]->t_last_update = c.t;

            // Настройка этого кодового слова для первого канала
            // 
            for(n = 0; n < numChannels; n++) {
                if(c.cb[i]->max[n] < *(p+n)) {
                    c.cb[i]->max[n] = *(p+n);
                } else if(c.cb[i]->min[n] > *(p+n)) {
                    c.cb[i]->min[n] = *(p+n);
                }
            }
            
            break;
        }
    }
 
// ... продолжение следует
```

Эта функция расширяет или добавляет запись в кодовую книгу, когда пиксель *p* выходит за границы существующего *box* кодовой книги. Если пиксель находится за пределами *cbBounds*, то создается новый *box* кодовой книги. В начале, функция задает значения уровней *high* и *low*, которые будут использованы позже. Затем происходит перебор каждой записи кодовой книги, чтобы проверить находится ли значение пикселя \**p* внутри пределов исследуемого "box" кодовой книги. Если пиксель находится внутри исследуемой границы для всех каналов, тогда соответствующие уровни *max* и *min* корректируются для того, чтобы включить данный пиксель, а время последнего обновления устанавливается в соответствии с текущим временным отсчетом *c.t*. Затем *update_codebook()* подсчитывает статистику обращений к записям кодовой книги:

```cpp
// ... продолжение
	
	// Накладные расходы для отслеживания потенциально устаревших записей
	//
    for(int s = 0; s < c.numEntries; s++) {
        // Отслеживание устаревших записей
        //
        int negRun = c.t - c.cb[s]->t_last_update;
        if(c.cb[s]->stale < negRun) {
            c.cb[s]->stale = negRun;
        }
    }

// ... продолжение следует
```

В результате переменная *stale* будет содержать наиболее *negative runtime* (т.е. наибольший промежуток времени, за который не было обращений к данным). Отслеживание неиспользуемых записей позволяет удалять кодовые книги, которые были созданы из шума или движущихся объектов и, следовательно, имеющие тенденцию оставаться не используемыми на протяжении всего времени. На следующем этапе обучения *update_codebook()* создает новую кодовую книгу, если это необходимо:

```cpp
// ... продолжение

    // Ввод нового кодового слова по необходимости
    // если ни одно из существующих кодовых слов не найдено
    // 
    if(i == c.numEntries) { 
        code_element **foo = new code_element* [c.numEntries+1];
        
        for(int ii = 0; ii < c.numEntries; ii++) {
            foo[ii] = c.cb[ii];
        }

        foo[c.numEntries] = new code_element;
        
        if(c.numEntries) {
            delete [] c.cb;
        }
        
        c.cb = foo;
        
        for(n = 0; n < numChannels; n++) {
            c.cb[c.numEntries]->learnHigh[n] = high[n];
            c.cb[c.numEntries]->learnLow[n] = low[n];
            c.cb[c.numEntries]->max[n] = *(p+n);
            c.cb[c.numEntries]->min[n] = *(p+n);
        }

        c.cb[c.numEntries]->t_last_update = c.t;
        c.cb[c.numEntries]->stale = 0;
        c.numEntries += 1;
    }

// ... продолжение следует
```

И наконец, *update_codebook()* неспешно корректирует (добавляя по 1) обучаемые границы *learnHigh* и *learnLow*, если пиксель был найден за пределами порогов, но все еще в пределах границ *high* и *low*:

```cpp
// ... продолжение

    // Неспешная корректировка обучаемых границ
    //
    for(n=0; n<numChannels; n++) {
	    if(c.cb[i]->learnHigh[n] < high[n]) {
	        c.cb[i]->learnHigh[n] += 1;
	    }
	    
	    if(c.cb[i]->learnLow[n] > low[n]) {
	        c.cb[i]->learnLow[n] -= 1;
	    }
    }

    return(i);
}
```

Функция завершает свою работу возвращением индекса измененной кодовой книги. Для того чтобы вести обучение в присутствии движущихся фоновых объектов и для того чтобы не зависеть от шума необходимо удалять редко используемые во время обучения записи.

**Изучение модели фона при наличии движущихся объектов переднего плана**

Следующая функция *clear_stale_entries()* позволяет обучать модель в присутствии движущихся объектов. 

```cpp
/////////////////////////////////////////////////////////////////// 
// int clear_stale_entries(codeBook &c)
// Во время обучения, по истечению некоторого времени, периодически
// происходит очищение от устаревших записей кодовой книги
//
// c    Codebook для очистки
//
// Return
// 	количество удаленных записей
// 
int clear_stale_entries(codeBook &c) {
    int staleThresh = c.t>>1;
    int *keep = new int [c.numEntries];
    int keepCnt = 0;

    // Просмотр устаревших записей кодовой книги
    //
    for(int i = 0; i < c.numEntries; i++){
        if(c.cb[i]->stale > staleThresh)
            keep[i] = 0; // Отметка удаления
        else {
            keep[i] = 1; // Отметка сохранения keepCnt += 1;
        }
    }

    // Сохранение только хороших
    //
    c.t = 0; // Сброс прошлых слежений
    code_element **foo = new code_element* [keepCnt]; int k=0;
    
    for(int ii=0; ii < c.numEntries; ii++){
        if(keep[ii]) {
            foo[k] = c.cb[ii];

            // Необходимо обновить записи до следующего clearStale 
            // 
            foo[k]->t_last_update = 0;
            k++;
        }
    }

    // Очистка
    //
    delete [] keep; 
    delete [] c.cb;
    c.cb = foo;
    int numCleared = c.numEntries - keepCnt; 
    c.numEntries = keepCnt; 

    return(numCleared);
}
```

Функция начинает свою работу с определения переменной *staleThresh*, которая жестко хранит половину общего времени работы *c.t*. Это означает, что если за время обучения к записи *i* кодовой книги не происходило обращение в течение половины времени работы, то запись *i* помечается для удаления (*keep[i] = 0*). Длина вектора *keep[]* равна *c.numEntries*, чтобы существовала возможность отметить каждую запись кодовой книги. Переменная *keepCnt* хранит количество записей, которые необходимо оставить. После сохранения оставленных записей в кодовой книге, происходит создание указателя *foo* на вектор указателей *code_elements* длиной *keepCnt* с последующим копированием в него устаревших записей. В заключении удаляется старый указатель на вектор кодовой книги и заменяется новым, не устаревшим вектором.

**Вычитание фона: поиск объектов переднего плана**

На данный момент уже были рассмотрено создание кодовой книги фона и её чистка от устаревших элементов. Теперь необходимо рассмотреть функцию *background_diff()*, которая использует обученную модель для отделения пикселей переднего плана от фона:

```cpp
////////////////////////////////////////////////////////////
// uchar background_diff( uchar *p, codeBook &c,
// int minMod, int maxMod)
// Данная функция определяет, является ли пиксель частью кодовой книги 
//
// p            Указатель на пиксель (YUV interleaved)
// c            Ссылка на codebook
// numChannels  Число каналов
// maxMod       Добавление (возможно отрицательного) числа к уровню 
//              max при определении, что новый пиксель является частью объекта
//              переднего плана 
// minMod       Вычитание (возможно отрицательного) числа из уровня 
//              min при определении, что новый пиксель является частью объекта 
//              переднего плана 
// NOTES:
// 	длины minMod и maxMod должна быть numChannels,
// 	т.е. 3 канала => minMod[3], maxMod[3]. При этом по одному min и 
// 	одному max порогу на канал.
//
// Return
// 	0 => фон, 255 => объект переднего плана
//
uchar background_diff(
     uchar*      p
    ,codeBook&   c
    ,int         numChannels
    ,int*        minMod
    ,int*        maxMod
) {
    int matchChannel;

    // Проверка принадлежности к существующему кодовому слову
    //
    for(int i=0; i<c.numEntries; i++) {
        matchChannel = 0;
        for(int n=0; n<numChannels; n++) {
            if((c.cb[i]->min[n] - minMod[n] <= *(p+n)) &&
            (*(p+n) <= c.cb[i]->max[n] + maxMod[n])) {
                matchChannel++; // Количество найденных записей для данного канала
            } else {
                break;
            }
        }
        
        if(matchChannel == numChannels) {
            break; // Найдена запись, соответствующая всем каналам
        }
    }

    if(i >= c.numEntries) {
        return(255);
    }

    return(0);
}
```

Функция вычитания фона похожа на функцию обучения *update_codebook()*, за исключением того факта, что в данной функции рассматриваются *max* и *min* плюс смещение порога, *maxMod* и *minMod* для каждого *box* кодовой книги. Если пиксель в пределах *box* плюс *maxMod* для верхней границы и минус *minMod* для нижней для каждого канала, то происходит инкремент переменной *matchChannel*. Фиксация записи, соответствующая всем каналам, происходит, когда значение *matchChannel* равно числу каналов. Если пиксель в пределах изучаемого *box*, тогда возвращается значение 255 (соответствует объектам переднего плана), иначе 0 (фон).

Три функции: *update_codebook()*, *clear_stale_elements()* и *background_diff()* составляют основу метода отделения переднего плана от фона с помощью кодовой книги.

**Использование модели фона при помощи кодовой книги**

Чтобы использовать метод отделения фона при помощи кодовой книги, необходимо выполнить следующие действия:

1. Обучить модель фона в течение нескольких секунд или минут, используя *update_codebook()*.
2. Избавиться от устаревших записей при помощи *clear_stale_entries()*.
3. Настроить пороги *minMod* и *maxMod* для лучшего отделения переднего плана.
4. Поддерживать модель сцены на высоком уровне.
5. Использовать обученную модель для отделения переднего плана от фона при помощи *background_diff()*.
6. Периодически обновлять модель.
7. Удалять устаревшие записи кодовой книги при помощи *clear_stale_entries()*.

**Еще несколько размышлений по поводу модели кодовой книги**

В основном, метод кодового словаря хорошо работает с обширным набором условий и его относительно легко обучить и запустить. Этот метод не очень хорошо работает при изменчивом освещении, т.е. когда происходит смена утра, дня и вечера или если кто-то включает и выключает свет в помещении. Этот тип глобальной изменчивости может быть учтен с помощью нескольких различных моделей кодового словаря, по одной для каждого условия, с последующим контролированием активной модели.


### Связанные компоненты для очистки объектов переднего плана

Прежде, чем перейти к сравнению метода усреднения и метода кодовой книги, необходимо рассмотреть пути очистки изображения, используя связанные компоненты. Данная форма анализа основывается на исходном шумовом изображении маски; при этом используется морфологическая операция *открытия* для сокращения небольших шумовых областей с последующим применением операции *закрытия* для восстановления областей, которые были удалены операцией открытия. После этого появляется возможность найти "достаточно большие" контуры сохранившихся сегментов и (необязательно) собрать статистику по всем таким сегментам. При этом появляется возможность в получении самого большого контура или всех контуров выше определенного порога. Далее будет представлено большинство функций, которые могут потребоваться при работе со связными компонентами:

* Аппроксимация уцелевших компонентов контура к полигонам или выпуклым оболочкам
* Установка, насколько большим должен быть контур, чтобы не быть удаленным
* Установка максимального числа возвращаемых контуров
* (Необязательно) Возвращение *bounding boxes* сохранившихся контуров
* (Необязательно) Возвращение центров сохранившихся контуров

Заголовок связной компоненты, которая реализует данные операции:

```cpp
///////////////////////////////////////////////////////////////////
// void find_connected_components(IplImage *mask, int poly1_hull0,
//                                float perimScale, int *num,
//                                CvRect *bbs, CvPoint *centers)
// Эта функция очищает сегмент маски, полученный при вызове backgroundDiff,
// от объектов переднего плана 
//
// mask Серая (глубиной 8-bit) "строка" изображения маски, которую 
//      необходимо очистить
//
// OPTIONAL PARAMETERS:
// poly1_hull0  Если установлен, то аппроксимация связной компоненты до 
//              полигона или выпуклой оболочки (0)
//              (DEFAULT: polygon)
// perimScale   Len = image (width+height)/perimScale. Если длина контура
//              len < this, тогда удалить данный контур 
//              (DEFAULT: 4)
// num          Максимальное число возвращаемых прямоугольников и/или центров;
//              (DEFAULT: NULL)
// bbs          Указатель на bounding box прямоугольных векторов длинною num
//              (DEFAULT SETTING: NULL)
// centers      Указатель на вектор центров контура длинною num
//              (DEFAULT: NULL)
//
void find_connected_components(
     IplImage*  mask
    ,int        poly1_hull0 = 1
    ,float      perimScale  = 4
    ,int*       num         = NULL
    ,CvRect*    bbs         = NULL
    ,CvPoint*   centers     = NULL
);
```

Разбор тела данной функции представлен ниже. В начале, задается хранилище под связные компоненты контура. Затем применяются морфологические операции открытия и закрытия для удаления небольших шумовых пикселей, после чего происходит восстановление разрушенных областей, которые сохранились после операции открытия. Функция принимает два дополнительных параметра, которые были жестко объявлены при помощи *#define*. Значения по умолчанию дают хорошие результаты и их вряд ли придётся когда-либо менять. Эти дополнительные параметры управляют тем, насколько простой должна быть граница переднего плана (чем выше число, тем проще) и сколько раз необходимо выполнять морфологические операции; чем выше число, тем выше размытие при выполнении операции открытия перед расширением во время выполнения операции закрытия. (Стоит отметить, что значение *CVCLOSE_ITR* на самом деле зависит от разрешения. Для изображений с очень высоким разрешением, значение по умолчанию равное 1, скорее всего не даст хороших результатов). Большое размытие устраняет крупные регионы шума за счет размытия границ у более крупных регионов. Параметры, используемые в приведенном коде, были подобранны опытным путем и дают довольно таки хорошие результаты, однако, никто не запрещает с ними поэкспериментировать.

```cpp
// Для связанных компонент:
// Approx.threshold - чем больше, тем проще граница
//
#define CVCONTOUR_APPROX_LEVEL 2

// Сколько итераций размытия и/или расширения должно быть
//
#define CVCLOSE_ITR 1
```

Теперь можно перейти непосредственно к рассмотрению самого алгоритма. Первая часть подпрограммы выполняет морфологические операции открытия и закрытия:

```cpp
void find_connected_components(
     IplImage*  mask
    ,int        poly1_hull0
    ,float      perimScale
    ,int*       num
    ,CvRect*    bbs
    ,CvPoint*   centers
) {
    static CvMemStorage*    mem_storage = NULL;
    static CvSeq*           contours    = NULL;

    // Очистка строки маски
    //
    cvMorphologyEx( mask, mask, 0, 0, CV_MOP_OPEN, CVCLOSE_ITR );
    cvMorphologyEx( mask, mask, 0, 0, CV_MOP_CLOSE, CVCLOSE_ITR );
```

Теперь, после удаления шумов из маски, можно найти все контуры:

```cpp
    // Поиск контуров только в больших регионах
    //
    if( mem_storage == NULL ) {
        mem_storage = cvCreateMemStorage(0);
    } else {
        cvClearMemStorage(mem_storage);
    }

    CvContourScanner scanner = cvStartFindContours(
         mask
        ,mem_storage
        ,sizeof(CvContour)
        ,CV_RETR_EXTERNAL
        ,CV_CHAIN_APPROX_SIMPLE
    );
```

Далее производится отсеивание контуров, которые слишком малы, и аппроксимация остальных до полигонов или выпуклых областей (чья сложность задается в *CVCONTOUR_APPROX_LEVEL*):

```cpp
    CvSeq*  c;
    int     numCont = 0;

    while( (c = cvFindNextContour( scanner )) != NULL ) {
        double len = cvContourPerimeter( c );

        // Вычисление периметра
        //
        double q = (mask->height + mask->width)/perimScale;

        // Отсеивание blob, если его периметр слишком мал
        //
        if( len < q ) {
            cvSubstituteContour( scanner, NULL );
        } else {
            // Сглаживание краев, если достаточно большие
            //
            CvSeq* c_new;
            if( poly1_hull0 ) {
                // Polygonal approximation
                //
                c_new = cvApproxPoly(
                     c
                    ,sizeof(CvContour)
                    ,mem_storage
                    ,CV_POLY_APPROX_DP
                    ,CVCONTOUR_APPROX_LEVEL
                    ,0
                );
            } else {
                // Convex Hull of the segmentation
                //
                c_new = cvConvexHull2(
                     c
                    ,mem_storage
                    ,CV_CLOCKWISE
                    ,1
                );
            }
            cvSubstituteContour( scanner, c_new );
            numCont++;
        }
    }

    contours = cvEndFindContours( &scanner );
```

В предыдущем коде, *CV_POLY_APPROX_DP* подразумевает использование алгоритма приближения *Douglas-Peucker*, а *CV_CLOCKWISE* по умолчанию подразумевает выпуклую оболочку контура. Все эти обработки в результате приводят к получению списка контуров. Перед нанесением контуров обратно на маску, необходимо определить цвета:

```cpp
	// Некоторые вспомогательные переменные
	// 
	const CvScalar CVX_WHITE = CV_RGB(0xff,0xff,0xff)
	const CvScalar CVX_BLACK = CV_RGB(0x00,0x00,0x00)
```

Эти определения будут использованы в следующем коде, где впервые произойдет обнуление маски с последующим нанесением контуров на маску. Кроме того, будет выполнена проверка потребности в сборе статистики по контурам (*bounding boxes* и центры):

```cpp
    // PAINT THE FOUND REGIONS BACK INTO THE IMAGE
    //
    cvZero( mask );
    IplImage *maskTemp;

    // CALC CENTER OF MASS AND/OR BOUNDING RECTANGLES
    //
    if(num != NULL) {
        // Пользователь хочет собирать статистику
        //
        int N = *num, numFilled = 0, i=0;
        CvMoments moments;
        double M00, M01, M10;
        maskTemp = cvCloneImage(mask);
        
        for(i = 0, c = contours; c != NULL; c = c->h_next, i++ ) {
            if(i < N) {
                // Only process up to *num of them
                //
                cvDrawContours(
                     maskTemp
                    ,c
                    ,CVX_WHITE
                    ,CVX_WHITE
                    ,-1
                    ,CV_FILLED
                    ,8
                );

                // Поиск центра для каждого контура
                //
                if(centers != NULL) {
                    cvMoments( maskTemp, &moments, 1 );
                    M00 = cvGetSpatialMoment( &moments, 0, 0 );
                    M10 = cvGetSpatialMoment( &moments, 1, 0 );
                    M01 = cvGetSpatialMoment( &moments, 0, 1 );
                    centers[i].x = (int)(M10/M00);
                    centers[i].y = (int)(M01/M00);
                }

                // Ограничивающие прямоугольники вокруг blobs
                //
                if(bbs != NULL) {
                    bbs[i] = cvBoundingRect(c);
                }
                
                cvZero(maskTemp);
                numFilled++;    
            }

            // Рисование контуров на маске
            //
            cvDrawContours(
                 mask
                ,c
                ,CVX_WHITE
                ,CVX_WHITE
                ,-1
                ,CV_FILLED
                ,8
            );
        }

        *num = numFilled;
        cvReleaseImage( &maskTemp);
	}
```

Если пользователю не нужны *bounding boxes* и центры в конечных регионах маски, необходимо просто отбросить контуры, представляющие достаточно большие связанные компоненты фона.

```cpp
    // Иначе просто нарисовать обработанные контуры на маске
    //
    else {
        // Пользователю не нужна статистика, просто нарисовать контуры
        //
        for( c = contours; c != NULL; c = c->h_next ) {
            cvDrawContours(
                 mask
                ,c
                ,CVX_WHITE
                ,CVX_BLACK
                ,-1
                ,CV_FILLED
                ,8
            );
        }
    }
}
```

Это все, что касается процедуры по созданию очищенных от шума масок. Теперь можно провести небольшой сравнительный анализ представленных методов вычитания фона.

**Быстрый тест**

Итак, рассмотрим пример работы алгоритма в реальной обстановке. Будем придерживаться видео с деревом за окном. Вспомним (рисунок 9-1), что в определенный момент времени перед камерой проводят рукой. Найти руку относительно легко, например, при помощи метода кадровой разности (метод был рассмотрен ранее). Основная идея метода заключается в вычитании текущего кадра из предыдущего и применение порога к разнице.

Последовательные кадры в видео, как правило, почти одинаковые. Следовательно, можно предположить, что в результате вычитания можно получить не так много информации, как, если бы на переднем плане появился движущийся объект. (В контексте кадровой разности, объект идентифицируется как "передний план", в большей степени скоростной. Это разумно только в сценах, которые, как правило, статичны или в сценах, где объекты переднего плана находятся гораздо ближе к камере, чем фоновые объекты (и, соответственно, передвигаться быстрее в силу проективной геометрии камер)). Но что означает "не так много" в данном контексте? На самом деле это означает "просто шум". На практике основная проблема заключается в удалении шума при наличии объекта переднего плана.

Что бы лучше осознать, что это за шум, рассмотрим пару кадров из видео, без объектов переднего плана – только фон и шум. На изображении 9-5 показан типичный кадр из видео (сверху слева) и предыдущий кадр (сверху справа). На рисунке так же представлен результат вычитания кадров со значением порога равным 15 (снизу слева). Можно отметить, что значительная часть шума принадлежит движущимся листьям дерева. Тем не менее, метод связанных компонент способен довольно таки хорошо удалить рассеянный шум (снизу справа). (Порог связанной компоненты был подобран таким образом, чтобы получить нулевой отклик в пустых кадрах. Остается лишь выяснить, останется ли объект переднего плана (рука) при таком значении порога. Рисунок 9-6 дает ответ на этот вопрос. И это не удивительно, потому что нет оснований ожидать пространственных корреляций, а сигнал характеризуется большим числом очень малых регионов.

![Рисунок 9-5 не найден](Images/Pic_9_5.jpg)

Рисунок 9-5. Межкадровая разность: раскачивающееся дерево на фоне в текущем (сверху слева) и предыдущем (сверху справа) кадрах; разность кадров (снизу слева) полностью очищенная (снизу справа) при помощи метода связанных компонент

Теперь рассмотрим ситуацию с объектом переднего плана (наша вездесущая рука) перемещающаяся перед камерой. На рисунке 9-6 показаны два кадра, схожие с кадрами с рисунка 9-5, за исключением того, что теперь на них присутствует перемещающаяся рука слева направо. Как и прежде показан текущий (сверху слева) и предыдущий (сверху справа) кадры, разность кадров (снизу слева) очищенная (снизу справа) при помощи метода связанной компоненты. Результаты довольно-таки хорошие.

![Рисунок 9-6 не найден](Images/Pic_9_6.jpg)

Рисунок 9-6. Использование метод межкадровой разности для обнаружения руки, которая перемещается слева направо и является объектом переднего плана (два верхних окна); изображение разности кадров (снизу слева) отображает "дыру" (где рука должна находиться) между её положением слева и передним краем справа и изображение, полученное методом связанной компоненты (снизу справа) 

При всем этом стоит отметить один недостаток метода вычитания фона: он не отличает регионы, где объект перемещается ("дыра") и где находится в настоящий момент времени. Более того, в перекрывающихся регионах зачастую образуется разрыв, так как "тело минус тело" это 0 (или, по крайней мере, ниже порога).

В результате, метод связанных компонент — это хороший инструмент для удаления шума при вычитании фона. В качестве бонуса, были рассмотрены сильные и слабые стороны метода межкадровой разности.


### Сравнение моделей фона

В этой главе были представлены два метода моделирования фона: метод усреднения и метод кодовой книги. Возможно, будет интересно знать, какой из методов лучше или, по крайней мере, какой проще в использовании. В таком случае лучше всего сравнить имеющиеся алгоритмы между собой.

Продолжим использовать видео с деревом. В дополнение к движущемуся дереву, на видео присутствуют блики от здания справа и от части внутренней стены слева. Это довольно-таки сложная модель фона.

На рисунке 9-7 приведено сравнение метода средней разности (сверху) и метода кодовой книги (снизу); необработанное изображение переднего плана (слева) очищенное при помощи метода связанной компоненты (справа). Как можно заметить, метод средней разности не затрагивает "небрежные" маски и разделяет руку на две составляющие. И это не удивительно; как было показано на рисунке 9-2 при использовании метода средней разности модель фона зачастую включает значения пикселей, связанных со значениями руки (показано пунктирной линией). Сравните это с рисунком 9-4, где метод кодовой книги может более точно смоделировать колебания листьев и веток и более точно отделить пиксели переднего плана (пунктирная линия) от пикселей заднего плана. Рисунок 9-7 подтверждает не только тот факт, что модель фона дает меньше шума, но также и то, что связанные компоненты могут генерировать достаточно точные контуры объекта.

![Рисунок 9-7 не найден](Images/Pic_9_7.jpg)

Рисунок 9-7. Метод усреднения (верхний ряд), очищенные связанные компоненты с исключением пальцев (сверху справа); метод кодовой книги (нижний ряд) дает более лучшую сегментацию и создает очищенную маску связанных компонентов (снизу справа)

