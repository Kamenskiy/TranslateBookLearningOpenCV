## [П]|[РС]|(РП) Другие алгоритмы машинного обучения

Итак, на данный момент должно уже сформироваться некоторое представление о том, как работает библиотека ML в OpenCV. Кроме всего прочего, она устроена таким образом, что позволяет с легкостью внедрить в неё новые алгоритмы и методы. И кстати, в ближайшее время планируется добавление новых алгоритмов. В данном разделе будет представлено краткое описание новых алгоритмов, которые уже добавлены в OpenCV на момент написания книги. Каждый из рассматриваемых алгоритмов реализует уже хорошо известную технику обучения, описание которой для конкретного алгоритма можно найти в других книгах, публикациях или интернете. Более подробную информацию по источникам можно найти в директории *.../opencv/docs/ref/opencvref_ml.htm*.


### Expectation Maximization

*Expectation maximization* (EM) является одной из наиболее популярной техники кластеризации. OpenCV поддерживает EM только с гауссовыми смесями, хотя сама техника носит более общий характер. Техника состоит из нескольких итераций, каждая из которых принимает наиболее вероятное (среднее или "ожидаемое") предположение текущей модели, с последующей настройкой этой модели для максимизации шансов, что предположение окажется верным. В OpenCV алгоритм EM реализован в классе *CvEM*, который использует годные гауссовы смеси. Т.к. пользователь сам предоставляет годное количество гауссиан, то можно сказать, что данный алгоритм схож с K-means.


### K-Nearest Neighbors

*K-Nearest Neighbors* (K-ближайших соседей) является одним из наиболее простых методов классификации, который просто сохраняет все изученные наблюдения. При возникновении необходимости классифицировать новое наблюдение, ищутся K её ближайших соседей (где K целое число), и новому наблюдению присваивается тот класс, который наиболее распространён среди найденных соседей. В OpenCV данный алгоритм реализован в классе *CvKNearest*. Техника KNN может быть очень эффективной, хоть и требует сохранения всего набора для обучения, что в свою очередь может привести к разрастанию памяти и как следствие сильному замедлению. Люди зачастую кластеризуют набор для обучения перед использованием данной техники с целью уменьшения его размера. Для читателей, которым интересно как методы динамически адаптивных ближайших соседей могут быть использованы в электронном мозгу (и в машинном обучении) могут изучить труд *Grossberg* или относительно более новый труд *Carpenter* и *Grossberg*.


### Multilayer Perceptron

*Multilayer perceptron* (многослойный персептрон, MLP; так же известный как *back-propagation*) - это нейронная сеть, которая на момент написания книги входила в число самых эффективных классификаторов, особенно в случаях распознавания текста. Обучение может быть довольно таки медленным, т.к. используется градиентный спуск для минимизации ошибки путем корректировки взвешенных связей между пронумерованными узлами классификации в пределах слоя. Однако в режиме тестирования, работает довольно таки быстро. Реализован данный классификатор в классе *CvANN_MLP*; пример использования можно найти в файле *.../opencv/samples/c/letter_recog.cpp*. Читатели, которые заинтересованы в эффективном использовании MLP для распознавания текста и объектов, найдут ответы в трудах LeCun, Bottou, Bengio и Haffner. Подробную информацию о реализации и настройке MLP можно найти у LeCun, Bottou и Muller. Относительно более новый труд о *мозгоподобных иерархических сетях* с распространяющейся вероятностью можно найти в работах Hinton, Osindero и Teh.


### Support Vector Machine

Имея большой объем данных, лучше всего использовать boosting или random trees классификатор. Однако, при наличии ограниченного набора данных *support vector machine* (SVM) зачастую работает лучше. Этот N-классовый алгоритм работает путём проецирования данных в многомерном пространстве (создаёт новое выходное измерение, комбинируя особенности) с последующим поиском оптимального линейного разделителя между классами. В исходном пространстве исходных данных этот многомерный линейный классификатор может стать далеко нелинейным. Следовательно, можно использовать методы линейной классификации, основанные на *максимальном межклассовом разделении*, для получения нелинейного классификатора, чтобы в каком-то смысле оптимально разделить классы. При наличии необходимого количества измерений, почти всегда возможно отлично разделить классы. Представленная техника реализована в классе *CvSVM*.

Представленный инструментарий тесно взаимодействует со многими алгоритмами компьютерного зрения, от поиска особенностей при помощи обученного классификатора для слежения, сегментирования сцен, до более простых задач классификации объектов и кластеризации изображений.

